---
title: "Is Reward enough for intelligence?"
subtitle: "Interesting works"
draft: true

---

Hey folks! :)

It is the first of a series of posts about my scientific readings.
There are a sort of *extended abstracts* extracted from them.
I present works also with some personal thoughts about the work.

## Reward is enough
### David Silver, Satinder Singh, Doina Precup, **Richard S, Sutton**

In the last years, artificial intelligence fame is grown exponentially. In particular, Reinforcement Learning (RL) is one of the most promising areas in which an *agent* tries to maximise a *reward* given by a stochastic and not fully known *environment*. 
This framework is general enough, so researchers and practitioners leverage it in a wide range of applications, starting from videogames up to smart grids.
Though the question poses here is not about its general appliance but about how reward-based signal maximisation can derive to intelligence.

### What is intelligence?
The authors take the McCharthy definition:
> Intelligence is the computational part of the ability to achieve goals in the worlds

It is worth noting that intelligence is a blurry term and an all-encompassing definition does not exist. Authors keep that one because it is quite related to the reinforcement learning framework due to its goal-seeking nature.

Here the intuition: 
> **Hypothesis** (Reward is enogh): Intelligence, and its associated abilities, can be understood as subserving the maximisation of reward by an agent acting in its environment

It implies that is exist a good reward-maximising agent then we can assume that it yields capabilities associated with intellingence. That makes for me this congetture very affascinating! 

